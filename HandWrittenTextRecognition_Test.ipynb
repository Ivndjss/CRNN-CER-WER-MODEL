{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (2220642534.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 15\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(pytesseract.image_to_string(Image.open('C:\\Users\\Arcly-za B Aguinaldo\\Desktop\\Test Pytesserect\\AA-001_1.jpg')))\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "\n",
    "# Set the Tesseract path (replace with your actual path)\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\\\Program Files (x86)\\\\Tesseract-OCR\\\\tesseract'\n",
    "\n",
    "image_path = 'C:\\\\Users\\\\Arcly-za B Aguinaldo\\\\Desktop\\\\Test Pytesserect\\\\AA-001_1.jpg'\n",
    "print(pytesseract.image_to_string(Image.open(image_path)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2825142583.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    which python  # or `where python` on Windows\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "print(\"pytesseract is installed and updated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class HandwritingDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert('L')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.get_label(image)  # Get label using OCR\n",
    "        return image, label\n",
    "\n",
    "    def get_label(self, image):\n",
    "        # Use Tesseract to extract text from image\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, imgH, nc, nclass, nh):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(nc, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.rnn = nn.Sequential(\n",
    "            nn.LSTM(128, nh, bidirectional=True),\n",
    "            nn.LSTM(nh * 2, nh, bidirectional=True)\n",
    "        )\n",
    "        self.fc = nn.Linear(nh * 2, nclass)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv = self.cnn(x)\n",
    "        b, c, h, w = conv.size()\n",
    "        assert h == 1, \"the height of conv must be 1\"\n",
    "        conv = conv.squeeze(2)\n",
    "        conv = conv.permute(2, 0, 1)\n",
    "        output, _ = self.rnn(conv)\n",
    "        output = self.fc(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=25):\n",
    "    for epoch in range(num_epochs):\n",
    "        for images, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            # Assuming labels are already in a suitable format for CTCLoss\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch}/{num_epochs}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader\n",
    "image_dir = r'C:\\Users\\Arcly-za B Aguinaldo\\Desktop\\Datasets\\Datasets (first batch)'\n",
    "transform = transforms.Compose([transforms.Resize((32, 128)), transforms.ToTensor()])\n",
    "dataset = HandwritingDataset(image_dir, transform)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, Loss, Optimizer\n",
    "model = CRNN(imgH=32, nc=1, nclass=37, nh=256)  # Adjust 'nclass' based on your character set\n",
    "criterion = nn.CTCLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "train_model(model, dataloader, criterion, optimizer, num_epochs=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Convert Dataset to IAM Format\n",
    "def convert_to_iam_format(image_dir, model, transform):\n",
    "    iam_data_dir = \"IAM_dataset\"\n",
    "    os.makedirs(iam_data_dir, exist_ok=True)\n",
    "    \n",
    "    for idx, img_file in enumerate(os.listdir(image_dir)):\n",
    "        if img_file.endswith('.png'):\n",
    "            img_path = os.path.join(image_dir, img_file)\n",
    "            image = Image.open(img_path).convert('L')\n",
    "            image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "            output = model(image)\n",
    "            pred_text = decode_output(output)  # Decode the model's output into text\n",
    "            \n",
    "            # Create IAM format file\n",
    "            iam_file_path = os.path.join(iam_data_dir, f'{img_file}.txt')\n",
    "            with open(iam_file_path, 'w') as f:\n",
    "                f.write(pred_text)\n",
    "\n",
    "def decode_output(output):\n",
    "    # This function will decode the CRNN model output into readable text\n",
    "    # Implement decoding logic here (e.g., Greedy/Beam Search, etc.)\n",
    "    decoded_text = \"decoded text from model output\"\n",
    "    return decoded_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to IAM format after training\n",
    "convert_to_iam_format(image_dir, model, transform)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
